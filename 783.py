import warnings
from pathlib import Path

import numpy as np
import pandas as pd
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor
from sklearn.linear_model import RidgeCV
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBRegressor

warnings.filterwarnings("ignore")

RANDOM_STATE = 42
DATA_DIR = Path("./data")


# ---------------------------------------------------------------------------
# 1. ê¸°ë³¸ ì „ì²˜ë¦¬ & Stage1
# ---------------------------------------------------------------------------
def enrich_time_features(df: pd.DataFrame) -> pd.DataFrame:
    def band_of_hour(hour: int) -> str:
        if (22 <= hour <= 23) or (0 <= hour <= 7):
            return "ê²½ë¶€í•˜"
        if 16 <= hour <= 21:
            return "ìµœëŒ€ë¶€í•˜"
        return "ì¤‘ê°„ë¶€í•˜"

    ref_date = pd.Timestamp("2024-10-24")
    df = df.copy()
    df["ì¸¡ì •ì¼ì‹œ"] = pd.to_datetime(df["ì¸¡ì •ì¼ì‹œ"], errors="coerce")
    df = df.sort_values("ì¸¡ì •ì¼ì‹œ").reset_index(drop=True)
    df["ì›”"] = df["ì¸¡ì •ì¼ì‹œ"].dt.month
    df["ì¼"] = df["ì¸¡ì •ì¼ì‹œ"].dt.day
    df["ìš”ì¼"] = df["ì¸¡ì •ì¼ì‹œ"].dt.dayofweek
    df["ì‹œê°„"] = df["ì¸¡ì •ì¼ì‹œ"].dt.hour
    df["ì£¼ë§ì—¬ë¶€"] = (df["ìš”ì¼"] >= 5).astype(int)
    df["ê²¨ìš¸ì—¬ë¶€"] = df["ì›”"].isin([11, 12, 1, 2]).astype(int)
    df["period_flag"] = (df["ì¸¡ì •ì¼ì‹œ"] >= ref_date).astype(int)
    df["sin_time"] = np.sin(2 * np.pi * df["ì‹œê°„"] / 24)
    df["cos_time"] = np.cos(2 * np.pi * df["ì‹œê°„"] / 24)
    df["ë¶€í•˜êµ¬ë¶„"] = df["ì‹œê°„"].apply(band_of_hour)
    return df


def encode_categorical(train: pd.DataFrame, test: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:
    train = train.copy()
    test = test.copy()
    le_job = LabelEncoder()
    train["ì‘ì—…ìœ í˜•_encoded"] = le_job.fit_transform(train["ì‘ì—…ìœ í˜•"].astype(str))
    test["ì‘ì—…ìœ í˜•_encoded"] = test["ì‘ì—…ìœ í˜•"].astype(str)
    test["ì‘ì—…ìœ í˜•_encoded"] = test["ì‘ì—…ìœ í˜•_encoded"].apply(
        lambda x: x if x in le_job.classes_ else le_job.classes_[0]
    )
    test["ì‘ì—…ìœ í˜•_encoded"] = le_job.transform(test["ì‘ì—…ìœ í˜•_encoded"])

    le_band = LabelEncoder()
    train["ë¶€í•˜êµ¬ë¶„_encoded"] = le_band.fit_transform(train["ë¶€í•˜êµ¬ë¶„"].astype(str))
    test["ë¶€í•˜êµ¬ë¶„_encoded"] = test["ë¶€í•˜êµ¬ë¶„"].astype(str)
    test["ë¶€í•˜êµ¬ë¶„_encoded"] = test["ë¶€í•˜êµ¬ë¶„_encoded"].apply(
        lambda x: x if x in le_band.classes_ else le_band.classes_[0]
    )
    test["ë¶€í•˜êµ¬ë¶„_encoded"] = le_band.transform(test["ë¶€í•˜êµ¬ë¶„_encoded"])

    train["ì‹œê°„_ì‘ì—…ìœ í˜•"] = train["ì‹œê°„"].astype(str) + "_" + train["ì‘ì—…ìœ í˜•_encoded"].astype(str)
    test["ì‹œê°„_ì‘ì—…ìœ í˜•"] = test["ì‹œê°„"].astype(str) + "_" + test["ì‘ì—…ìœ í˜•_encoded"].astype(str)
    le_combo = LabelEncoder()
    train["ì‹œê°„_ì‘ì—…ìœ í˜•_encoded"] = le_combo.fit_transform(train["ì‹œê°„_ì‘ì—…ìœ í˜•"])
    test["ì‹œê°„_ì‘ì—…ìœ í˜•_encoded"] = test["ì‹œê°„_ì‘ì—…ìœ í˜•"].apply(
        lambda x: x if x in le_combo.classes_ else le_combo.classes_[0]
    )
    test["ì‹œê°„_ì‘ì—…ìœ í˜•_encoded"] = le_combo.transform(test["ì‹œê°„_ì‘ì—…ìœ í˜•_encoded"])
    return train, test


def run_stage1(train: pd.DataFrame, test: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:
    targets = [
        "ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)",
        "ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)",
        "ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)",
        "ì§€ìƒì—­ë¥ (%)",
        "ì§„ìƒì—­ë¥ (%)",
    ]
    features = [
        "ì›”",
        "ì¼",
        "ìš”ì¼",
        "ì‹œê°„",
        "ì£¼ë§ì—¬ë¶€",
        "ê²¨ìš¸ì—¬ë¶€",
        "period_flag",
        "sin_time",
        "cos_time",
        "ì‘ì—…ìœ í˜•_encoded",
        "ë¶€í•˜êµ¬ë¶„_encoded",
        "ì‹œê°„_ì‘ì—…ìœ í˜•_encoded",
    ]
    model_factory = {
        "ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)": lambda: LGBMRegressor(
            n_estimators=2000, learning_rate=0.02, num_leaves=128, random_state=RANDOM_STATE
        ),
        "ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)": lambda: LGBMRegressor(
            n_estimators=1800, learning_rate=0.02, num_leaves=96, random_state=RANDOM_STATE
        ),
        "ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)": lambda: LGBMRegressor(
            n_estimators=1800, learning_rate=0.02, num_leaves=96, random_state=RANDOM_STATE
        ),
        "ì§€ìƒì—­ë¥ (%)": lambda: LGBMRegressor(
            n_estimators=1500, learning_rate=0.02, num_leaves=64, random_state=RANDOM_STATE
        ),
        "ì§„ìƒì—­ë¥ (%)": lambda: LGBMRegressor(
            n_estimators=1500, learning_rate=0.02, num_leaves=64, random_state=RANDOM_STATE
        ),
    }
    stage1_oof = pd.DataFrame(index=train.index)
    stage1_test = pd.DataFrame(index=test.index)

    for target in targets:
        model = model_factory[target]()
        oof_pred = np.zeros(len(train))
        test_pred = np.zeros(len(test))
        fold_sizes = []

        months_sorted = sorted(train["ì›”"].unique())
        for month in months_sorted:
            train_mask = train["ì›”"] < month
            val_mask = train["ì›”"] == month
            if train_mask.sum() == 0 or val_mask.sum() == 0:
                continue
            X_tr = train.loc[train_mask, features]
            y_tr = train.loc[train_mask, target]
            X_va = train.loc[val_mask, features]
            model.fit(X_tr, y_tr)
            oof_pred[val_mask] = model.predict(X_va)
            fold_sizes.append(val_mask.sum())
        zero_mask = oof_pred == 0
        model.fit(train[features], train[target])
        if zero_mask.any():
            oof_pred[zero_mask] = model.predict(train.loc[zero_mask, features])
        test_pred = model.predict(test[features])
        stage1_oof[target] = oof_pred
        stage1_test[target] = test_pred
    return stage1_oof, stage1_test


# ---------------------------------------------------------------------------
# 2. Stage2 Feature Engineering
# ---------------------------------------------------------------------------
def add_power_factor_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["ì§€ìƒì—­ë¥ _ë³´ì •"] = df["ì§€ìƒì—­ë¥ (%)"].clip(lower=60, upper=100)
    df["ì£¼ê°„ì—¬ë¶€"] = df["ë¶€í•˜êµ¬ë¶„"].isin(["ì¤‘ê°„ë¶€í•˜", "ìµœëŒ€ë¶€í•˜"]).astype(int)
    df["ë²•ì í˜ë„í‹°"] = ((df["ì§€ìƒì—­ë¥ _ë³´ì •"] < 90) & (df["ì£¼ê°„ì—¬ë¶€"] == 1)).astype(int)
    df["ì‹¤ì§ˆìœ„í—˜"] = ((df["ì§€ìƒì—­ë¥ _ë³´ì •"] < 94) & (df["ì£¼ê°„ì—¬ë¶€"] == 1)).astype(int)
    df["ê·¹ì €ì—­ë¥ "] = ((df["ì§€ìƒì—­ë¥ _ë³´ì •"] < 85) & (df["ì£¼ê°„ì—¬ë¶€"] == 1)).astype(int)
    df["ì—­ë¥ ë¶€ì¡±í­_94"] = (94 - df["ì§€ìƒì—­ë¥ _ë³´ì •"]).clip(lower=0)
    df["ìœ íš¨ì—­ë¥ (%)"] = df[["ì§€ìƒì—­ë¥ (%)", "ì§„ìƒì—­ë¥ (%)"]].max(axis=1)
    df["ë¬´íš¨ìœ íš¨ë¹„ìœ¨"] = df["ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)"] / (df["ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)"] + 1e-6)
    df["ë¶€í•˜ì—­ë¥ ê³±"] = df["ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)"] * df["ì—­ë¥ ë¶€ì¡±í­_94"]
    df["ì—­ë¥ ë‹¹ì „ë ¥"] = df["ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)"] / (df["ì§€ìƒì—­ë¥ _ë³´ì •"] + 1e-6)
    df["ê°€ì„ìœ„í—˜"] = ((df["ì›”"].isin([9, 10])) & (df["ì‹¤ì§ˆìœ„í—˜"] == 1)).astype(int)
    df["ë™ì ˆê¸°ì•ˆì •"] = ((df["ê²¨ìš¸ì—¬ë¶€"] == 1) & (df["ì§€ìƒì—­ë¥ _ë³´ì •"] >= 94)).astype(int)
    df["ì—­ë¥ _ì›”í‰ê· "] = df.groupby("ì›”")["ì§€ìƒì—­ë¥ _ë³´ì •"].transform("mean")
    df["ì—­ë¥ _ì›”í‰ê· ì°¨ì´"] = df["ì§€ìƒì—­ë¥ _ë³´ì •"] - df["ì—­ë¥ _ì›”í‰ê· "]

    df["ì—­ë¥ _60_85"] = (
        (df["ì§€ìƒì—­ë¥ _ë³´ì •"] >= 60) & (df["ì§€ìƒì—­ë¥ _ë³´ì •"] < 85) & (df["ì£¼ê°„ì—¬ë¶€"] == 1)
    ).astype(int)
    df["ì—­ë¥ _85_90"] = (
        (df["ì§€ìƒì—­ë¥ _ë³´ì •"] >= 85) & (df["ì§€ìƒì—­ë¥ _ë³´ì •"] < 90) & (df["ì£¼ê°„ì—¬ë¶€"] == 1)
    ).astype(int)
    df["ì—­ë¥ _90_94"] = (
        (df["ì§€ìƒì—­ë¥ _ë³´ì •"] >= 90) & (df["ì§€ìƒì—­ë¥ _ë³´ì •"] < 94) & (df["ì£¼ê°„ì—¬ë¶€"] == 1)
    ).astype(int)
    df["ì—­ë¥ _94_ì´ìƒ"] = ((df["ì§€ìƒì—­ë¥ _ë³´ì •"] >= 94) & (df["ì£¼ê°„ì—¬ë¶€"] == 1)).astype(int)

    df["ì—­ë¥ _94cut_gap"] = df["ì—­ë¥ ë¶€ì¡±í­_94"].copy()
    df["ì „ë ¥x94cut"] = df["ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)"] * df["ì—­ë¥ _94cut_gap"]
    df["kvarh_x94cut"] = df["ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)"] * df["ì—­ë¥ _94cut_gap"]
    df["ì „ë ¥xì£¼ê°„ìœ„í—˜"] = df["ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)"] * df["ì‹¤ì§ˆìœ„í—˜"]
    df["ì „ë ¥xì—­ë¥ ë¶€ì¡±"] = df["ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)"] * df["ì—­ë¥ ë¶€ì¡±í­_94"]
    df["kvarh_xì—­ë¥ ë¶€ì¡±"] = df["ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)"] * df["ì—­ë¥ ë¶€ì¡±í­_94"]
    return df


def add_sequence_features(train: pd.DataFrame, test: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:
    seq_cols = [
        "ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)",
        "ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)",
    ]
    combined = (
        pd.concat(
            [
                train.assign(_dataset="train"),
                test.assign(_dataset="test"),
            ],
            ignore_index=True,
        )
        .sort_values("ì¸¡ì •ì¼ì‹œ")
        .reset_index(drop=True)
    )
    combined["kwh_lag24"] = combined["ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)"].shift(24)
    combined["kwh_roll24_mean"] = combined["ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)"].shift(1).rolling(24).mean()
    combined["kwh_roll24_std"] = (
        combined["ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)"].shift(1).rolling(24).std().fillna(0)
    )
    combined["kwh_ë³€í™”ìœ¨_24h"] = (
        (combined["ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)"] - combined["kwh_lag24"]) / (combined["kwh_lag24"] + 1e-6)
    )

    combined["kvarh_lag24"] = combined["ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)"].shift(24)
    combined["kvarh_roll24_mean"] = (
        combined["ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)"].shift(1).rolling(24).mean()
    )
    combined["kvarh_roll24_std"] = (
        combined["ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)"].shift(1).rolling(24).std().fillna(0)
    )
    combined["ì „ë ¥í’ˆì§ˆì§€ìˆ˜"] = (
        combined["kvarh_roll24_mean"] / (combined["kwh_roll24_mean"] + 1e-6)
    )

    train_enhanced = combined.loc[combined["_dataset"] == "train"].drop(columns="_dataset")
    test_enhanced = combined.loc[combined["_dataset"] == "test"].drop(columns="_dataset")
    return train_enhanced.reset_index(drop=True), test_enhanced.reset_index(drop=True)


# ---------------------------------------------------------------------------
# 3. Custom CV & Sample Weights
# ---------------------------------------------------------------------------
def create_custom_folds(train: pd.DataFrame) -> list[dict]:
    fold_specs = [
        (list(range(1, 7)), [7]),
        (list(range(2, 8)), [8]),
        (list(range(3, 9)), [9]),
        (list(range(4, 10)), [10]),
        (list(range(5, 11)), [11]),
    ]
    folds = []
    for train_months, val_months in fold_specs:
        train_mask = train["ì›”"].isin(train_months).values
        val_mask = train["ì›”"].isin(val_months).values
        if train_mask.sum() == 0 or val_mask.sum() == 0:
            continue
        folds.append(
            {
                "train_months": train_months,
                "val_months": val_months,
                "train_mask": train_mask,
                "val_mask": val_mask,
            }
        )
    return folds


def build_sample_weights(train: pd.DataFrame) -> np.ndarray:
    weights = np.ones(len(train))
    weights[train["ì›”"] == 7] *= 2.0
    weights[train["ì‹¤ì§ˆìœ„í—˜"] == 1] *= 1.5
    return weights


# ---------------------------------------------------------------------------
# 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (7ì›” ê¸°ì¤€)
# ---------------------------------------------------------------------------
def tune_model(
    model_name: str,
    model_cls,
    param_grid: list[dict],
    X: np.ndarray,
    y_log: np.ndarray,
    y_true: np.ndarray,
    sample_weights: np.ndarray,
    train_mask: np.ndarray,
    val_mask: np.ndarray,
) -> tuple[dict, float]:
    best_score = np.inf
    best_params = None
    for params in param_grid:
        model = model_cls(**params)
        if model_name == "xgb":
            model.set_params(eval_metric="mae")
        model.fit(
            X[train_mask],
            y_log[train_mask],
            sample_weight=sample_weights[train_mask],
        )
        pred = np.expm1(model.predict(X[val_mask]))
        score = mean_absolute_error(y_true[val_mask], pred)
        if score < best_score:
            best_score = score
            best_params = params
    return best_params, best_score


# ---------------------------------------------------------------------------
# 5. ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ---------------------------------------------------------------------------
def main():
    train = pd.read_csv(DATA_DIR / "train.csv")
    test = pd.read_csv(DATA_DIR / "test.csv")

    train = enrich_time_features(train)
    test = enrich_time_features(test)
    train, test = encode_categorical(train, test)

    stage1_oof, stage1_test = run_stage1(train, test)
    for col in stage1_oof.columns:
        train[col] = stage1_oof[col]
        test[col] = stage1_test[col]

    train = add_power_factor_features(train)
    test = add_power_factor_features(test)
    train, test = add_sequence_features(train, test)

    stage2_features = [
        "ì›”",
        "ì¼",
        "ìš”ì¼",
        "ì‹œê°„",
        "ì£¼ë§ì—¬ë¶€",
        "ê²¨ìš¸ì—¬ë¶€",
        "period_flag",
        "sin_time",
        "cos_time",
        "ì‘ì—…ìœ í˜•_encoded",
        "ë¶€í•˜êµ¬ë¶„_encoded",
        "ì‹œê°„_ì‘ì—…ìœ í˜•_encoded",
        "ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)",
        "ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)",
        "ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)",
        "ì§€ìƒì—­ë¥ (%)",
        "ì§„ìƒì—­ë¥ (%)",
        "ìœ íš¨ì—­ë¥ (%)",
        "ì§€ìƒì—­ë¥ _ë³´ì •",
        "ì£¼ê°„ì—¬ë¶€",
        "ì‹¤ì§ˆìœ„í—˜",
        "ì—­ë¥ ë¶€ì¡±í­_94",
        "ë¬´íš¨ìœ íš¨ë¹„ìœ¨",
        "ë¶€í•˜ì—­ë¥ ê³±",
        "ì—­ë¥ ë‹¹ì „ë ¥",
        "ê°€ì„ìœ„í—˜",
        "ë™ì ˆê¸°ì•ˆì •",
        "ì—­ë¥ _ì›”í‰ê· ",
        "ì—­ë¥ _ì›”í‰ê· ì°¨ì´",
        "ì—­ë¥ _60_85",
        "ì—­ë¥ _85_90",
        "ì—­ë¥ _90_94",
        "ì—­ë¥ _94_ì´ìƒ",
        "ì—­ë¥ _94cut_gap",
        "ì „ë ¥x94cut",
        "kvarh_x94cut",
        "ì „ë ¥xì£¼ê°„ìœ„í—˜",
        "ì „ë ¥xì—­ë¥ ë¶€ì¡±",
        "kvarh_xì—­ë¥ ë¶€ì¡±",
        "kwh_lag24",
        "kwh_roll24_mean",
        "kwh_roll24_std",
        "kwh_ë³€í™”ìœ¨_24h",
        "kvarh_lag24",
        "kvarh_roll24_mean",
        "kvarh_roll24_std",
        "ì „ë ¥í’ˆì§ˆì§€ìˆ˜",
    ]

    X_all = train[stage2_features].values
    X_te = test[stage2_features].values
    y_all = train["ì „ê¸°ìš”ê¸ˆ(ì›)"].values
    y_all_log = np.log1p(y_all)
    sample_weights = build_sample_weights(train)

    folds = create_custom_folds(train)
    print("ğŸ“… Custom CV êµ¬ì¡°:")
    for idx, fold in enumerate(folds, 1):
        print(f"  Fold{idx}: train_months={fold['train_months']} | val_months={fold['val_months']}")

    first_fold = folds[0]
    param_grid_lgb = [
        {"learning_rate": lr, "num_leaves": nl, "n_estimators": ne, "subsample": ss, "colsample_bytree": cb, "random_state": RANDOM_STATE}
        for lr in [0.015, 0.02, 0.03]
        for nl in [48, 64, 96]
        for ne in [1200, 1600]
        for ss, cb in [(0.8, 0.8), (0.9, 0.9)]
    ][:25]
    param_grid_xgb = [
        {"learning_rate": lr, "max_depth": md, "n_estimators": ne, "subsample": ss, "colsample_bytree": cb, "reg_lambda": rl, "reg_alpha": ra, "random_state": RANDOM_STATE}
        for lr in [0.015, 0.02, 0.03]
        for md in [5, 7, 9]
        for ne in [1200, 1600]
        for ss, cb in [(0.8, 0.8), (0.9, 0.9)]
        for rl, ra in [(3, 0), (5, 1)]
    ][:20]
    param_grid_cat = [
        {"depth": dp, "learning_rate": lr, "iterations": it, "l2_leaf_reg": l2, "random_state": RANDOM_STATE, "verbose": 0}
        for dp in [6, 8]
        for lr in [0.02, 0.03]
        for it in [1500, 2000]
        for l2 in [3, 5, 7]
    ][:15]

    best_lgb_params, best_lgb_mae = tune_model(
        "lgb",
        LGBMRegressor,
        param_grid_lgb,
        X_all,
        y_all_log,
        y_all,
        sample_weights,
        first_fold["train_mask"],
        first_fold["val_mask"],
    )
    best_xgb_params, best_xgb_mae = tune_model(
        "xgb",
        XGBRegressor,
        param_grid_xgb,
        X_all,
        y_all_log,
        y_all,
        sample_weights,
        first_fold["train_mask"],
        first_fold["val_mask"],
    )
    if best_xgb_params is not None:
        best_xgb_params = {**best_xgb_params, "eval_metric": "mae"}
    best_cat_params, best_cat_mae = tune_model(
        "cat",
        CatBoostRegressor,
        param_grid_cat,
        X_all,
        y_all_log,
        y_all,
        sample_weights,
        first_fold["train_mask"],
        first_fold["val_mask"],
    )

    print("\nğŸ¯ íŠœë‹ ê²°ê³¼ (7ì›” MAE ê¸°ì¤€):")
    print(f"  LightGBM best MAE={best_lgb_mae:.2f} | params={best_lgb_params}")
    print(f"  XGBoost  best MAE={best_xgb_mae:.2f} | params={best_xgb_params}")
    print(f"  CatBoost best MAE={best_cat_mae:.2f} | params={best_cat_params}")

    oof_preds = pd.DataFrame(index=train.index, columns=["lgb", "xgb", "cat"], dtype=float)
    test_preds = {"lgb": np.zeros(len(test)), "xgb": np.zeros(len(test)), "cat": np.zeros(len(test))}

    for fold in folds:
        tr_mask = fold["train_mask"]
        va_mask = fold["val_mask"]

        lgb_model = LGBMRegressor(**best_lgb_params)
        lgb_model.fit(X_all[tr_mask], y_all_log[tr_mask], sample_weight=sample_weights[tr_mask])
        oof_preds.loc[va_mask, "lgb"] = lgb_model.predict(X_all[va_mask])
        test_preds["lgb"] += lgb_model.predict(X_te) / len(folds)

        xgb_model = XGBRegressor(**best_xgb_params)
        xgb_model.fit(
            X_all[tr_mask],
            y_all_log[tr_mask],
            sample_weight=sample_weights[tr_mask],
        )
        oof_preds.loc[va_mask, "xgb"] = xgb_model.predict(X_all[va_mask])
        test_preds["xgb"] += xgb_model.predict(X_te) / len(folds)

        cat_model = CatBoostRegressor(**best_cat_params)
        cat_model.fit(
            X_all[tr_mask],
            y_all_log[tr_mask],
            sample_weight=sample_weights[tr_mask],
        )
        oof_preds.loc[va_mask, "cat"] = cat_model.predict(X_all[va_mask])
        test_preds["cat"] += cat_model.predict(X_te) / len(folds)

    meta_learner = RidgeCV(alphas=np.logspace(-3, 3, 20), fit_intercept=True)
    oof_valid = oof_preds.dropna()
    meta_learner.fit(oof_valid.values, y_all_log[oof_valid.index])

    oof_stacked = np.expm1(meta_learner.predict(oof_valid.values))
    oof_pred_series = pd.Series(oof_stacked, index=oof_valid.index, name="pred")
    stacking_mae = mean_absolute_error(y_all[oof_valid.index], oof_stacked)
    stacking_r2 = r2_score(y_all[oof_valid.index], oof_stacked)
    print(f"\nğŸ“Š ìµœì¢… Stacking OOF: MAE={stacking_mae:.2f} | RÂ²={stacking_r2:.4f}")

    oof_valid_months = train.loc[oof_pred_series.index, "ì›”"]
    monthly_mae = {}
    for month in sorted(oof_valid_months.dropna().unique()):
        month_idx = oof_valid_months[oof_valid_months == month].index
        if month_idx.empty:
            continue
        month_mae = mean_absolute_error(y_all.loc[month_idx], oof_pred_series.loc[month_idx])
        monthly_mae[int(month)] = float(month_mae)
    if monthly_mae:
        print("\nğŸ“† ì›”ë³„ OOF MAE:")
        for month in sorted(monthly_mae):
            print(f"  {month}ì›” MAE={monthly_mae[month]:.2f}")
    else:
        print("\nâš ï¸ ì›”ë³„ OOF MAEë¥¼ ê³„ì‚°í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    test_stack_input = pd.DataFrame(test_preds)
    pred_test = np.expm1(meta_learner.predict(test_stack_input.values))
    submission = pd.DataFrame({"id": test["id"], "target": pred_test})
    submission.to_csv("submission_final_custom_cv.csv", index=False)

    lgb_full = LGBMRegressor(**best_lgb_params)
    lgb_full.fit(X_all, y_all_log, sample_weight=sample_weights)
    feature_importance = pd.DataFrame(
        {
            "feature": stage2_features,
            "importance": lgb_full.feature_importances_,
        }
    ).sort_values("importance", ascending=False)
    top30 = feature_importance.head(30)
    print("\nğŸ” Top 30 Feature Importance (LightGBM):")
    print(top30.to_string(index=False))
    new_feature_set = {
        "ì—­ë¥ _94cut_gap",
        "ì „ë ¥x94cut",
        "kvarh_x94cut",
        "ì „ë ¥xì£¼ê°„ìœ„í—˜",
        "ì „ë ¥xì—­ë¥ ë¶€ì¡±",
        "kvarh_xì—­ë¥ ë¶€ì¡±",
    }
    present_new = sorted(f for f in new_feature_set if f in top30["feature"].values)
    print(f"\nğŸ†• ì‹ ê·œ í”¼ì²˜ Top30 í¬í•¨: {', '.join(present_new) if present_new else 'None'}")


if __name__ == "__main__":
    main()
